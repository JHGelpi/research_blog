# Research: Software Engineering Practices and Trends

*Date: 2025-10-30*

## Research Focus

This week's research focuses on Software Engineering practices in late 2025, with particular emphasis on:
- AI's impact on developer productivity and workflows
- How engineering teams are adapting their practices
- Platform engineering maturity
- Evolution of engineering leadership priorities
- Testing and code review practices in the AI era

## Key Articles & Sources

### Article 1: The State of Developer Ecosystem 2025
- **Source**: JetBrains Research Blog
- **Link**: https://blog.jetbrains.com/research/2025/10/state-of-developer-ecosystem-2025/
- **Published**: October 2025
- **Key Takeaways**:
  - 85% of developers regularly use AI tools; 62% rely on at least one AI coding assistant
  - Developer productivity now measured by both technical (51%) and non-technical (62%) factors
  - 66% of developers don't believe current metrics reflect their true contributions
  - Concerns persist: inconsistent AI code quality, limited understanding of complex logic
  - 61% of junior developers find the job market challenging vs. 54% of senior developers
- **Relevance**: Shows the gap between how organizations measure productivity and what developers actually experience. Critical for managers understanding team effectiveness.
- **Potential Angles**: "Are we measuring the wrong things?" / "The productivity metrics disconnect" / "What developers actually need to be productive"

### Article 2: Developer Productivity AI Arena Launch
- **Source**: JetBrains Blog
- **Link**: https://blog.jetbrains.com/blog/2025/10/28/the-launch-of-developer-productivity-ai-arena-an-open-platform-for-benchmarking-ai-coding-agents/
- **Published**: October 28, 2025
- **Key Takeaways**:
  - New open-source platform for benchmarking AI coding tools on real-world tasks
  - Current benchmarks rely on outdated datasets and narrow technology coverage
  - Evaluates tools across patching, bug fixing, PR review, test generation, and static analysis
  - Addresses the gap between tools that accelerate work vs. those that genuinely understand development
- **Relevance**: As teams adopt AI tools, they need ways to evaluate which actually improve productivity vs. just speed. Critical for tool selection decisions.
- **Potential Angles**: "Beyond the hype: How to actually evaluate AI coding tools" / "What makes an AI coding assistant actually useful?"

### Article 3: How Engineering Leadership is Changing in 2025
- **Source**: LeadDev
- **Link**: https://leaddev.com/reporting/how-engineering-leadership-is-changing-in-2025
- **Published**: June 19, 2025
- **Key Takeaways**:
  - Engineering leadership adapting to tightening budgets and rise of generative AI
  - Focus shifting to efficiency while maintaining team motivation
  - Communicating change, reorganizing teams, and preventing burnout are increasingly critical skills
  - Priorities shifting alongside team structures and required leadership skills
- **Relevance**: Engineering managers face dual pressure: deliver more with less while keeping teams engaged. Directly addresses the reality many in the audience are living.
- **Potential Angles**: "The new engineering manager mandate" / "Leading through efficiency pressure" / "What engineering teams actually need from leaders now"

### Article 4: Platform Engineering Trends for 2025
- **Source**: Multiple industry sources (DuploCloud, Mia-Platform, Configu)
- **Link**: https://duplocloud.com/emerging-trends-in-platform-engineering-for-2025/
- **Published**: 2025
- **Key Takeaways**:
  - 55% of organizations have adopted platform engineering; 80% expected by 2026 (Gartner)
  - Shift from "developer portals first" to "APIs and orchestration first"
  - 75% of developers lose 6+ hours weekly due to tool fragmentation
  - Platform teams struggle to prove business impact and are often labeled cost centers
  - AI-driven automation embedded into platform development lifecycle
- **Relevance**: Platform engineering promised to solve developer productivity but many implementations aren't delivering. Teams need to understand what actually works.
- **Potential Angles**: "Why your developer portal isn't getting adopted" / "Platform engineering reality check" / "What platform teams get wrong"

### Article 5: AI Revolution in Software Development (October 2025)
- **Source**: Coaio
- **Link**: https://coaio.com/news/2025/10/ai-revolution-in-software-development-key-updates-from-october-2025-and-their-industry-impact/
- **Published**: October 2025
- **Key Takeaways**:
  - Microsoft released Agent Framework preview (Oct 2) for .NET and Python
  - Anthropic's Claude Sonnet 4.5 achieves 77.2% on SWE-bench for software engineering tasks
  - Teams using Gen AI tools complete tasks 21% faster
  - Only about half of developers "somewhat" trust AI responses; 30% trust "a little" or "not at all"
- **Relevance**: Major AI releases in October show rapid advancement, but trust remains an issue. Teams need to navigate hype vs. reality.
- **Potential Angles**: "The AI trust gap in software engineering" / "Speed vs. trust: The AI productivity paradox"

### Article 6: Test Automation Trends for 2025
- **Source**: Multiple testing industry sources (TestGuild, Xray, TestRail)
- **Link**: https://testguild.com/automation-testing-trends/
- **Published**: 2025
- **Key Takeaways**:
  - AI testing adoption increased from 7% (2023) to 16% (2025)
  - 72.3% of teams exploring or adopting AI-driven testing workflows
  - DevOps adoption in testing: 51.8% (2024) up from 16.9% (2022)
  - 26% of teams replacing up to 50% of manual testing; 20% replacing 75%+
  - Gartner predicts 70% of new applications will use low-code/no-code testing tools
- **Relevance**: Testing is often the bottleneck in delivery. AI and automation promise relief but adoption is still early. Testers need to understand what's actually working.
- **Potential Angles**: "Is AI testing ready for production?" / "The testing automation gap"

### Article 7: Code Review Practices in 2025
- **Source**: Multiple dev tools sources (Qodo, Swarmia, GitHub)
- **Link**: https://www.qodo.ai/blog/code-review-best-practices/
- **Published**: 2025
- **Key Takeaways**:
  - Many teams now use AI co-reviewers alongside human reviewers
  - PRs wait 4.4 days on average to get reviewed in traditional teams
  - Teams lose 20-40% of velocity to inefficient code review processes
  - Fast code review turnaround helps developers feel 20% more innovative
  - Forward-thinking teams maintain healthy skepticism toward AI reviews
- **Relevance**: Code review is a daily friction point. AI promises to help but teams need to balance speed with quality and learning.
- **Potential Angles**: "The code review bottleneck" / "Can AI actually help with code review?" / "Why code reviews still take too long"

## Emerging Themes

Several strong patterns emerge across these articles that speak to the current state of software engineering:

- **The Trust Gap**: While AI adoption is nearly universal (85% of developers), trust remains low. Only half "somewhat" trust AI outputs, and 66% of developers don't believe current metrics reflect their contributions. There's a disconnect between what's being measured/adopted and what actually builds confidence. (Articles 1, 2, 5, 7)

- **The Measurement Problem**: Organizations are investing heavily in productivity tools and metrics, yet developers feel increasingly unmeasured by the wrong things. The shift from purely technical metrics (51%) to including non-technical factors (62%) hasn't solved the fundamental problem: we're still not measuring what actually matters. (Articles 1, 3, 4)

- **Implementation vs. Promise**: Platform engineering, AI testing, and developer portals all show the same pattern: high adoption rates but disappointing results. 75% of developers lose 6+ hours weekly to tool fragmentation despite platform engineering adoption, and teams spend 12-18 months on developer portals with little adoption. The gap between what these solutions promise and what they deliver is widening. (Articles 4, 6)

- **The Speed-Quality Tension**: AI tools make things 21% faster, but code reviews still take 4.4 days and teams lose 20-40% of velocity to inefficient processes. The question isn't whether tools can accelerate individual tasks—it's whether they can address the systemic bottlenecks that actually slow teams down. (Articles 2, 5, 7)

- **Leadership Under Pressure**: Engineering leaders face a new mandate: deliver efficiency while maintaining motivation and preventing burnout. This dual pressure is reshaping what leadership skills matter most. (Article 3)

## Potential Blog Post Ideas

### Option 1: "The Productivity Paradox: Why We're Faster But Not Better"
- **Central Question**: If AI makes us 21% faster and 85% of developers use AI tools, why do 66% of developers feel their contributions aren't being measured correctly, and why are teams still losing 20-40% of velocity to inefficient processes?
- **Supporting Articles**: Articles 1 (JetBrains survey), 2 (AI Arena), 5 (AI updates), 7 (code review)
- **Audience Hook**: Every developer and manager is feeling this tension right now—more tools, more speed promises, but somehow still the same frustrations. This hits at the daily experience.
- **Estimated Scope**: Yes. Can explore the paradox through 2-3 concrete examples (AI coding, metrics, code review) and land on what actually moves the needle. 800-1000 words.

### Option 2: "What Platform Engineering Gets Wrong"
- **Central Question**: Why are teams spending 12-18 months building developer portals that nobody uses, and what should they be building instead?
- **Supporting Articles**: Article 4 (platform engineering), 1 (what developers actually need), 3 (leadership pressure to deliver efficiency)
- **Audience Hook**: Many organizations have platform engineering teams or are considering them. This addresses the gap between investment and outcomes.
- **Estimated Scope**: Yes. Can focus on the "portal-first" mistake, what developers actually need, and the shift to "APIs and orchestration first." 900-1100 words.

### Option 3: "The Metrics Disconnect"
- **Central Question**: How can we measure developer productivity when 66% of developers say current metrics don't reflect their contributions, and productivity now includes more non-technical (62%) than technical (51%) factors?
- **Supporting Articles**: Articles 1 (developer survey), 3 (leadership changes), 4 (platform teams can't prove impact)
- **Audience Hook**: Every engineering leader is being asked to prove productivity improvements. This speaks to the fundamental challenge of measurement.
- **Estimated Scope**: Yes. Explore what's being measured vs. what should be, with concrete examples of the gap. 800-1000 words.

### Option 4: "Trust, But Verify: The AI Coding Reality Check"
- **Central Question**: If only half of developers "somewhat" trust AI code and 30% trust it "a little" or "not at all," what does responsible AI adoption actually look like?
- **Supporting Articles**: Articles 1 (trust statistics), 2 (AI benchmarking), 5 (AI updates), 7 (AI in code review)
- **Audience Hook**: Nearly universal adoption with low trust is an unstable equilibrium. Teams need practical guidance on navigating this.
- **Estimated Scope**: Yes. Can focus on the trust gap, why it exists, and what "healthy skepticism" actually means in practice. 900-1100 words.

## Additional Notes

**Timing**: October 2025 has been a particularly rich month for software engineering news—major AI releases (Claude Sonnet 4.5, Microsoft Agent Framework), the JetBrains Developer Ecosystem Survey, and the AI Arena launch all provide fresh data points. This is good timing for a reflection piece.

**Strongest angle**: Option 1 (The Productivity Paradox) feels like the most compelling because it connects all the themes—AI adoption, measurement problems, speed vs. systemic bottlenecks—and speaks directly to what practitioners are experiencing daily.

**Data quality**: The JetBrains survey (24,534 developers) provides strong quantitative backing. The Gartner predictions and industry trend reports add credibility.

**Potential challenge**: Need to be careful not to be overly cynical about AI or new practices. The goal is "curious explorer" who asks good questions, not a skeptic dismissing innovation. The trust gap and implementation challenges are real, but so is the potential.
